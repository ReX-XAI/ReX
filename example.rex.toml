[rex]
# masking value for mutations, can be either an integer, float or
# one of the following built-in occlusions 'spectral', 'min', 'mean'
# mask_value = 0

# random seed, only set for reproducibility
# seed = 42

# whether to use gpu or not, defaults to true
# gpu = true

# batch size for the model
# batch_size = 64

[rex.onnx]
# means for min-max normalization
# means = [0.485, 0.456, 0.406]

# standards devs for min-max normalization
# stds = [0.229, 0.224, 0.225]

# binary model confidence threshold. Anything >= threshold will be classified as 1, otherwise 0
# binary_threshold = 0.5

[rex.visual]
# whether to show progress bar in the terminal, defalts to true
# progress_bar = false

# resize the explanation to the size of the original image. This uses cubic interpolation and will not be as visually accurate as not resizing, defaults to false
# resize = true

# include classification and confidence information in title of plot, defaults to true
# info = false

# produce unvarnished image with actual masking value, defaults to false
# raw = false

# pretty printing colour for explanations, defaults to 200
# colour = 150

# matplotlib colourscheme for responsibility map plotting, defaults to 'magma'
# heatmap = 'coolwarm'

# alpha blend for main image, defaults to 0.2 (PIL Image.blend parameter)
# alpha = 0.2

# overlay a 10*10 grid on an explanation, defaults to false
# grid = false

# mark quickshift segmentation on image
# mark_segments = false

[causal]
# maximum depth of tree, defaults to 10, note that search can actually go beyond this number on occasion, as the
# check only occurs at the end of an iteration
# tree_depth = 30

# limit on number of combinations to consider <per iteration>, defaults to none.
# It is **not** the total work done by ReX over all iterations. Leaving the search limit at none
# can potentially be very expensive.
# search_limit = 1000

# number of times to run the algorithm, defaults to 20
# iters = 30

# minimum child size, in pixels
# min_box_size = 10

# remove passing mutants which have a confidence less thatn <confidence_filter>. Defaults to 0.0 (meaning all mutants are considered)
# confidence_filter = 0.5

# whether to weight responsibility by prediction confidence, default to false
# weighted = false

# queue_style = "intersection" | "area" | "all" | "dc", defaults to "area"
# queue_style = "area"

# maximum number of things to hold in search queue, either an integer or 'all'
# queue_len = 1

# concentrate: weight responsibility by size and depth of passing partition. Defaults to false
# concentrate = false

[causal.distribution]
# distribution for splitting the box, defaults to uniform. Possible choices are 'uniform' | 'binom' | 'betabinom' | 'adaptive'
# distribution = 'uniform'

#blend = 0.5

# supplimental arguments for distribution creation, these are ignored if <distribution> does not take any parameters
# dist_args = [1.1, 1.1]

[explanation]
# iterate through pixel ranking in chunks, defaults to causal.min_box_size
# chunk = 10

[explanation.spatial]
# initial search radius
# initial_radius = 25

# increment to change radius
# radius_eta = 0.2

# number of times to expand before quitting, defaults to 10
# no_expansions = 50

[explanation.multi]
# multi method (just spotlight so far)
# method = 'spotlight'

# no of spotlights to launch
# spotlights = 10

# default size of spotlight
# spotlight_size = 24

# decrease spotlight by this amount
# spotlight_eta = 0.2

# objective function for spotlight search. Possible options 'mean' | 'max' | 'min'
# obj_function = 'mean'

[explanation.evaluation]

# insertion/deletion curve step size
# insertion_step = 100
